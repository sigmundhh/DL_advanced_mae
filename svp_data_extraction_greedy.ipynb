{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbef08f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "TRAIN_DIR = \"ImageNet1000/imagenet-mini/train\"\n",
    "OUTPUT_DIR = \"bananas/train\"\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "GREEDY_FILE_LOAD_BATCH_SIZE = 256\n",
    "SVP_EPOCHS = 50\n",
    "SVP_BATCH_SIZE = 64\n",
    "EXTRACTION_PERCENTAGE = 0.65 #0.02712 # range (0, 1)\n",
    "EXTRACTION_METHOD = \"even\" # best|even|rebalanced  - best simply takes topK, even is split roughly evenly among classes, rebalanced is balanced according to class distribution in train set\n",
    "DEBUG_BREAK = 0 # remove\n",
    "\n",
    "def calculate_entropy(preds):\n",
    "\tentropy = np.zeros(preds.shape[0])\n",
    "\n",
    "\tfor i in range(preds.shape[1]):\n",
    "\t\tentropy += preds[:, i] * np.log(preds[:, i])\n",
    "\n",
    "\treturn -entropy\n",
    "\n",
    "def define_model(height, width):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(height, width, 3)))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Dropout(0.3))\n",
    "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Dropout(0.4))\n",
    "\tmodel.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform', name=\"final_layer\"))\n",
    "\tmodel.add(BatchNormalization())\n",
    "\tmodel.add(Dropout(0.6))\n",
    "\tmodel.add(Dense(1000, activation='softmax'))\n",
    " \n",
    "\t# configure learning rate scheduler\n",
    "\tinitial_lr = 0.01\n",
    "\t# cosine decay with restarts of warm-up parameter\n",
    "\tfirst_decay_steps = 10000\n",
    "\tlr_scheduler = CosineDecayRestarts(initial_lr, first_decay_steps)\n",
    "\t# configure optimizer\n",
    "\topt = SGD(learning_rate=lr_scheduler, momentum=0.9)\n",
    "\t# compile model\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    " \n",
    "\treturn model\n",
    "\n",
    "def extract_indices(entropy_index, extraction_size, class_count, class_max, truth):\n",
    "  # method best\n",
    "  if class_count is None and class_max is None:\n",
    "    return entropy_index[:extraction_size]\n",
    "\n",
    "  # method even\n",
    "  if class_max is None:\n",
    "    num = int(np.ceil(extraction_size / len(class_count)))\n",
    "    total_count = 0\n",
    "    final_indices = []\n",
    "\n",
    "    for index in entropy_index:\n",
    "      cl = truth[index]\n",
    "      if class_count[cl] < num:\n",
    "        final_indices.append(index)\n",
    "        class_count[cl] += 1\n",
    "        total_count += 1\n",
    "\n",
    "      if total_count == extraction_size:\n",
    "        break\n",
    "\n",
    "    return np.array(final_indices)\n",
    "\n",
    "  # method balanced\n",
    "  total_count = 0\n",
    "  final_indices = []\n",
    "\n",
    "  for index in entropy_index:\n",
    "    cl = truth[index]\n",
    "    if class_count[cl] < num:\n",
    "      final_indices.append(index)\n",
    "      class_count[cl] += 1\n",
    "      total_count += 1\n",
    "\n",
    "    if total_count == extraction_size:\n",
    "      break\n",
    "\n",
    "  return np.array(final_indices)\n",
    "\n",
    "def gen_image_data(path, files, height, width):\n",
    "  image_data = []\n",
    "\n",
    "  for f in files:\n",
    "    dir = path + \"/\"\n",
    "    image_string = tf.io.read_file(dir + f)\n",
    "    image_decoded = tf.image.decode_jpeg(image_string)\n",
    "    image_resized = tf.image.resize(image_decoded, [height, width])\n",
    "\n",
    "    if image_resized.shape[2] == 1:\n",
    "      image_resized = tf.image.grayscale_to_rgb(image_resized)\n",
    "\n",
    "    image_data.append(image_resized)\n",
    "\n",
    "  return tf.convert_to_tensor(image_data)\n",
    "\n",
    "def gen_one_hot_labels(dirs, map, num_classes=1000):\n",
    "  labels = np.zeros((map[\"_stats\"][\"files_count\"], num_classes))\n",
    "  start_index = 0\n",
    "  end_index = 0\n",
    "\n",
    "  #DEBUG_BREAK = 0 # remove\n",
    "  for i, dir in enumerate(dirs):\n",
    "    end_index += map[dir][\"count\"]\n",
    "    labels[start_index:end_index, i] = 1.0\n",
    "    start_index += map[dir][\"count\"]\n",
    "\n",
    "  #  DEBUG_BREAK += 1 # remove\n",
    "  #  if DEBUG_BREAK == 5: # remove\n",
    "  #    break # remove\n",
    "\n",
    "  return labels\n",
    "\n",
    "def list_dir(directory):\n",
    "  dirs = os.listdir(directory)\n",
    "  classes = {dir: {key: None for key in [\"count\"]} for dir in dirs} # [\"class_num\", \"count\"]\n",
    "  classes[\"_stats\"] = {key: None for key in [\"min\", \"max\", \"avg\", \"median\", \"files_count\", \"class_count\"]}\n",
    "  files = {\"full_path\": [], \"class\": []} # , \"filename\": [] , \"class\": []\n",
    "  files_count = []\n",
    "\n",
    "  base_dir = directory + \"/\"\n",
    "  #DEBUG_BREAK = 0 # remove\n",
    "  for i, dir in enumerate(dirs):\n",
    "    file_list = os.listdir(base_dir + dir)\n",
    "    sub_dir = dir + \"/\"\n",
    "    #files[\"filename\"] += file_list\n",
    "    file_list = [sub_dir + f for f in file_list]\n",
    "    files[\"full_path\"] += file_list\n",
    "    files[\"class\"] += np.full(len(file_list), i).tolist()\n",
    "    classes[dir][\"count\"] = len(file_list)\n",
    "    #classes[dir][\"class_num\"] = i\n",
    "    files_count.append(len(file_list))\n",
    "\n",
    "    #DEBUG_BREAK += 1 # remove\n",
    "    #if DEBUG_BREAK == 5: # remove\n",
    "    #  break # remove\n",
    "\n",
    "  classes[\"_stats\"][\"min\"] = np.min(files_count)\n",
    "  classes[\"_stats\"][\"max\"] = np.max(files_count)\n",
    "  classes[\"_stats\"][\"avg\"] = np.mean(files_count)\n",
    "  classes[\"_stats\"][\"median\"] = np.median(files_count)\n",
    "  classes[\"_stats\"][\"files_count\"] = np.sum(files_count)\n",
    "  classes[\"_stats\"][\"class_count\"] = len(dirs)\n",
    "\n",
    "  return dirs, files, classes\n",
    "\n",
    "def create_new_dataset(files, indices, dirs, input_path, output_path, force_overwrite=False):\n",
    "  path_split = output_path.split(\"/\")\n",
    "  assert os.path.exists(input_path), \"input path non-existing\"\n",
    "\n",
    "  if output_path[len(output_path) - 1] == \"/\":\n",
    "    path_split = path_split[:-1]\n",
    "\n",
    "  if os.path.exists(output_path):\n",
    "    if force_overwrite:\n",
    "      shutil.rmtree(path_split[0])\n",
    "      os.makedirs(output_path)\n",
    "    else:\n",
    "      assert len(os.listdir(output_path)) == 0, \"output path exists and is not empty\"\n",
    "  else:\n",
    "    try:\n",
    "      os.makedirs(output_path)\n",
    "    except Exception:\n",
    "      print(\"Failed creating output dir\")\n",
    "\n",
    "  try:\n",
    "    for i, ind in enumerate(indices):\n",
    "      dir, filename = files[\"full_path\"][ind].split(\"/\")\n",
    "      dir_full = output_path + dir if output_path[len(output_path) - 1] == \"/\" else output_path + \"/\" + dir\n",
    "\n",
    "      if not os.path.exists(dir_full):\n",
    "        os.mkdir(dir_full)\n",
    "\n",
    "      source_path = input_path + dir + \"/\" + filename if input_path[len(input_path) - 1] == \"/\" else input_path + \"/\" + dir + \"/\" + filename\n",
    "      shutil.copy(source_path, dir_full)\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Failed to copy \" + dir + \"/\" + filename)\n",
    "\n",
    "\n",
    "sorted_dirs, files, class_map = list_dir(TRAIN_DIR)\n",
    "#train_x = gen_image_data(train_dir, files[\"full_path\"], img_height, img_width)\n",
    "labels = gen_one_hot_labels(sorted_dirs, class_map)\n",
    "\n",
    "total_files = len(files[\"full_path\"])\n",
    "total_classes = class_map[\"_stats\"][\"class_count\"]\n",
    "total_file_batches = total_files // GREEDY_FILE_LOAD_BATCH_SIZE if total_files % GREEDY_FILE_LOAD_BATCH_SIZE == 0 else total_files // GREEDY_FILE_LOAD_BATCH_SIZE + 1\n",
    "total_file_batches = 1\n",
    "SVP_EPOCHS = 1\n",
    "model = define_model(IMG_HEIGHT, IMG_WIDTH)\n",
    "\n",
    "print(\"Starting... Will run \" + str(SVP_EPOCHS) + \" SVP epochs and greedily load files in batches of \" + str(GREEDY_FILE_LOAD_BATCH_SIZE))\n",
    "print(\"Total number of files: \" + str(total_files) + \" | Found classes: \" + str(total_classes))\n",
    "\n",
    "start_time = time.time_ns()\n",
    "for epoch in range(SVP_EPOCHS):\n",
    "  print(\"Epoch \" + str(epoch + 1)  + \" out of \" + str(SVP_EPOCHS))\n",
    "  index_all = np.random.choice(total_files, size=total_files, replace=False)\n",
    "\n",
    "  for file_batch in range(total_file_batches):\n",
    "    print(\"File batch \" + str(file_batch + 1) + \" out of \" + str(total_file_batches))\n",
    "    indices = index_all[file_batch * GREEDY_FILE_LOAD_BATCH_SIZE:(file_batch + 1) * GREEDY_FILE_LOAD_BATCH_SIZE]\n",
    "    file_subset = np.array(files[\"full_path\"])[indices]\n",
    "    train_x = gen_image_data(TRAIN_DIR, file_subset, IMG_HEIGHT, IMG_WIDTH)\n",
    "    train_y = tf.convert_to_tensor(labels[indices])\n",
    "\n",
    "    # preprocess - normalize data\n",
    "    train_x = train_x / 255.0\n",
    "\n",
    "    # data augmentation\n",
    "    datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "    batch = datagen.flow(train_x, train_y, batch_size=train_x.shape[0], shuffle=False)\n",
    "    train_x, train_y = next(batch)\n",
    "\n",
    "    # SVP FIT\n",
    "    model.fit(train_x, train_y, epochs=1, batch_size=SVP_BATCH_SIZE, verbose=1)\n",
    "\n",
    "    train_x = None\n",
    "    train_y = None\n",
    "\n",
    "preds_all = []\n",
    "index = 0\n",
    "\n",
    "for file_batch in range(total_file_batches):\n",
    "  file_subset = np.array(files[\"full_path\"])[index:index + GREEDY_FILE_LOAD_BATCH_SIZE]\n",
    "  train_x = gen_image_data(TRAIN_DIR, file_subset, IMG_HEIGHT, IMG_WIDTH)\n",
    "  train_x = train_x / 255.0\n",
    "  preds = model.predict(train_x)\n",
    "\n",
    "  index += GREEDY_FILE_LOAD_BATCH_SIZE\n",
    "  preds_all += preds.tolist()\n",
    "\n",
    "entropy = calculate_entropy(np.array(preds))\n",
    "entropy_index = np.argsort(entropy)[::-1]\n",
    "\n",
    "print(\"Max entropy duration\", ((time.time_ns() - start_time) / 1000000000), \"seconds.\")\n",
    "\n",
    "extraction_size = int(np.floor(total_files * EXTRACTION_PERCENTAGE))\n",
    "class_count = None\n",
    "if EXTRACTION_METHOD == \"even\" or EXTRACTION_METHOD == \"rebalanced\":\n",
    "  class_count = np.zeros(total_classes)\n",
    "class_max = None\n",
    "if EXTRACTION_METHOD == \"rebalanced\":\n",
    "  class_max = np.zeros(total_classes)\n",
    "  for i in range(total_classes):\n",
    "    class_max[i] = EXTRACTION_PERCENTAGE * class_map[sorted_dirs[i]][\"count\"]\n",
    "\n",
    "  total_max = np.sum(class_max)\n",
    "  print(\"TOTALS\", extraction_size, total_max)\n",
    "\n",
    "  if total_max < extraction_size:\n",
    "    diff = extraction_size - total_max\n",
    "    index = np.argsort(class_max)[:diff]\n",
    "    class_max[index] += 1\n",
    "\n",
    "  if total_max > extraction_size:\n",
    "    diff = extraction_size - total_max\n",
    "    index = np.argsort(class_max)[::-1][:diff]\n",
    "    class_max[index] -= 1\n",
    "\n",
    "final_indices = extract_indices(entropy_index, extraction_size, class_count, class_max, files[\"class\"])\n",
    "\n",
    "###\n",
    "# DO SVP STUFF!\n",
    "# REMEMBER: DOES IT ALTER SORTING OF TRAINING DATA IN-PLACE?\n",
    "# PREPROCESS\n",
    "\n",
    "#entropy_index = [4, 56, 21, 106, 199, 3] # RANDOM TOY INDICES\n",
    "\n",
    "create_new_dataset(files, final_indices, sorted_dirs, TRAIN_DIR, OUTPUT_DIR, force_overwrite=True) # REMOVE OR FORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96429e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python shutdown.py 15"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
